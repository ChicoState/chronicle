---
title: "Data Analysis for Chronicle Project"
author: "Kevin Buffardi and Rahul Bijoor"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_float: false
    number_sections: true
    theme: readable
    highlight: textmate
    smart: true
    df_print: tibble
    code_folding: show
---


```{r setup, include=FALSE, error=FALSE, warning=FALSE, message=FALSE}
# Load required packages (hidden from knitted document)
library("readr")
library("tidyr")
library("dplyr")
library("ggplot2")
library("writexl")
library("caret")
library("tibble")
library("psych")
library("stringr")
library("DescTools")

knitr::opts_chunk$set(echo = TRUE)
```

First, we will load the anonymized GitHub and survey data:
```{r loading_data, message = FALSE, warning = FALSE}
github <- readr::read_csv("data/coded_collated_data.csv")
survey <- readr::read_csv("data/coded_survey_anonymous.csv")
```

At a glance, here are some descriptive statistics of our data:

  * Total number of teams: **`r github %>% select(Repository) %>% unique() %>% nrow()`**
  * Total number of students: **`r github %>% select(Author) %>% unique() %>% nrow()`**
  
Summarize team trends:
```{r team trends}
github_by_team <- github %>% select(Repository,Action) %>% pivot_wider(names_from = Action, values_from = Action, values_fn = length)
github_by_team <- github_by_team %>% mutate(total = commit+pull_request+issue+comment+code_review)
```
  * Trends, per team:
    * Commits (M = **`r github_by_team %>% pull(commit) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(commit) %>% sd(na.rm=TRUE)`**)
    * Pull requests (M = **`r github_by_team %>% pull(pull_request) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(pull_request) %>% sd(na.rm=TRUE)`**)
    * Issues (M = **`r github_by_team %>% pull(issue) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(issue) %>% sd(na.rm=TRUE)`**)
    * Comments (M = **`r github_by_team %>% pull(comment) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(comment) %>% sd(na.rm=TRUE)`**)
    * Code Reviews (M = **`r github_by_team %>% pull(code_review) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(code_review) %>% sd(na.rm=TRUE)`**)
    * Total artifacts (M = **`r github_by_team %>% pull(total) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(total) %>% sd(na.rm=TRUE)`**)
    
For each team, we need to calculate their aggregate score for **Conflict** and **Cohesiveness**, by finding the mean score of the answers of all of the members of the team (who responded).

  * **Conflict**. Conflict questions include:
    * `How frequently do you have disagreements within your work group about the task of the project you are working on?`
    * `How often do people in your work group have conflicting opinions about the project you are working on?`
    * `How much emotional conflict is there in your work group?`
    * `How often do people get angry while working in your group?`
    * `How much conflict of ideas is there in your work group?`
    * `How often do you disagree about resource allocation in your work group?`
    * `How much relationship tension is there in your work group?`
    * `How often are there disagreements about who should do what in your work group?`
    * `How much conflict is there in your group about task responsibilities?`
  * **Cohesiveness**. Cohesiveness questions include:
    * `Team members get to participate in enjoyable activities`
    * `Team members enjoy spending time together`
    * `Team members get along well`
    * `I'm unhappy with my team's level of commitment to the task` (inverse scale should be reversed by `6-x` where x is the raw value)
    * `Our team is united in trying to reach its goals for performance`
    * `Our team members have conflicting aspirations for the team's performance`
    * `Team members like each other`
    * `Team members like the work that the group does`
    * `Being part of the team allows team members to do enjoyable work`

```{r calc_team_survey_scores}
survey <- survey %>% 
  mutate(
    Inverted_Unhappy = 6 - `I'm unhappy with my team's level of commitment to the task`
  )
conflict_columns <- c(
  "How frequently do you have disagreements within your work group about the task of the project you are working on?",
  "How often do people in your work group have conflicting opinions about the project you are working on?",
  "How much emotional conflict is there in your work group?",
  "How often do people get angry while working in your group?",
  "How much conflict of ideas is there in your work group?",
  "How often do you disagree about resource allocation in your work group?",
  "How much relationship tension is there in your work group?",
  "How often are there disagreements about who should do what in your work group?",
  "How much conflict is there in your group about task responsibilities?"
)

cohesiveness_columns <- c(
  "Team members get to participate in enjoyable activities",
  "Team members enjoy spending time together",
  "Team members get along well",
  "Inverted_Unhappy",
  "Our team is united in trying to reach its goals for performance",
  "Our team members have conflicting aspirations for the team's performance",
  "Team members like each other",
  "Team members like the work that the group does",
  "Being part of the team allows team members to do enjoyable work"
)



aggregate_scores <- survey %>%
  group_by(`Your Team`) %>%  
  summarize(
    Conflict = mean(as.numeric(unlist(across(all_of(conflict_columns)))), na.rm = TRUE),
    Cohesiveness = mean(as.numeric(unlist(across(all_of(cohesiveness_columns)))), na.rm = TRUE)
  )

```


Then, we want to look at the descriptive statistics for these team conflict and cohesiveness scores:

  * Team conflict (M = `r mean(aggregate_scores %>% pull(Conflict), na.rm = TRUE) %>% round(2)`, sd = `r sd(aggregate_scores %>% pull(Conflict), na.rm = TRUE) %>% round(2)`)
  * Team cohesiveness (M = `r mean(aggregate_scores %>% pull(Cohesiveness), na.rm = TRUE) %>% round(2)`, sd = `r sd(aggregate_scores %>% pull(Cohesiveness), na.rm = TRUE) %>% round(2)`)
  
Next, we want to determine statistics about individuals:

```{r by individual}
# For each individual, how many artifacts did they contribute per week:
github_by_individual <- github %>% select(Repository,Author,week) %>% 
                        pivot_wider(names_from = week, values_from = week, values_fn = length) %>%
                        replace(is.na(.), 0) %>%
                        mutate(activity=(`1`+`2`+`3`+`4`+`5`+`6`+`7`+`8`+`9`+`10`+`11`+`12`+`13`+`14`+`15`+`16`))
github_by_individual <- github_by_individual %>%
                        mutate(across(!activity & is.numeric, ~1 * (. > 0))) %>% 
                        mutate(consistency = (`1`+`2`+`3`+`4`+`5`+`6`+`7`+`8`+`9`+`10`+`11`+`12`+`13`+`14`+`15`+`16`)/16) %>%
                        select(Repository,Author,consistency,activity)
```

  * the ratio of *number of weeks an individual has an artifact* to the *total number of weeks their team had any activity during the semester* (from 0.0 to 1.0), from here on out referred to as **"consistency"** (M = `r github_by_individual %>% pull(consistency) %>% mean(na.rm=TRUE)`, sd = `r github_by_individual %>% pull(consistency) %>% sd(na.rm=TRUE)`)
  * the total number of activities per individual (M = `r github_by_individual %>% pull(activity) %>% mean(na.rm=TRUE)`, sd = `r github_by_individual %>% pull(activity) %>% sd(na.rm=TRUE)`)

```{r individuals_artifacts}
team_gini <- github_by_individual %>%
    group_by(Repository) %>%
    summarize(
        Gini_Artifacts = Gini(activity),
        Gini_Consistency = Gini(consistency)
    )

```

To determine the (in)equality within each team, we use [`Gini` from `DescTools`](https://search.r-project.org/CRAN/refmans/DescTools/html/Gini.html) to calculate the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient). Low values (approaching 0) indicate equality while high values (approaching 1) represent inequality.

  * Team Gini for total number of artifacts (M = `r team_gini %>% pull(Gini_Artifacts) %>% mean(na.rm = TRUE)`, sd = `r team_gini %>% pull(Gini_Artifacts) %>% sd(na.rm = TRUE)`)
  * Team Gini for consistency (M = `r team_gini %>% pull(Gini_Consistency) %>% mean(na.rm = TRUE)`, sd = `r team_gini %>% pull(Gini_Consistency) %>% sd(na.rm = TRUE)`)
  
```{r correlations}
team_gini$Repository <- trimws(team_gini$Repository)
aggregate_scores$`Your Team` <- trimws(aggregate_scores$`Your Team`)
team_outcomes <- merge(x=team_gini,y=aggregate_scores, by.x = "Repository", by.y = "Your Team", all=TRUE)
quantity_v_conflict <- cor.test(team_outcomes$Gini_Artifacts,team_outcomes$Conflict, method = "spearman")
quantity_v_cohesiveness <- cor.test(team_outcomes$Gini_Artifacts,team_outcomes$Cohesiveness, method = "spearman")
consistency_v_conflict <- cor.test(team_outcomes$Gini_Consistency,team_outcomes$Conflict, method = "spearman")
consistency_v_cohesiveness <- cor.test(team_outcomes$Gini_Consistency,team_outcomes$Cohesiveness, method = "spearman")
```

What is the correlation between:

  * Team Gini for total number of artifacts *vs* Team conflict (rho=`r quantity_v_conflict$estimate`, p=`r quantity_v_conflict$p.value`)
  * Team Gini for total number of artifacts *vs* Team cohesiveness (rho=`r quantity_v_cohesiveness$estimate`, p=`r quantity_v_cohesiveness$p.value`)
  * Team Gini for consistency *vs* Team conflict (rho=`r consistency_v_conflict$estimate`, p=`r consistency_v_conflict$p.value`)
  * Team Gini for consistency *vs* Team cohesiveness (rho=`r consistency_v_cohesiveness$estimate`, p=`r consistency_v_cohesiveness$p.value`)
  
```{r}
colnames(team_gini)
colnames(aggregate_scores)
setdiff(team_gini$Repository, aggregate_scores$`Your Team`)
setdiff(aggregate_scores$`Your Team`, team_gini$Repository)

```
```{r linear models}
model_conflict <- lm(team_outcomes$Conflict ~ team_outcomes$Gini_Artifacts + team_outcomes$Gini_Consistency)
model_cohesiveness <- lm(team_outcomes$Cohesiveness ~ team_outcomes$Gini_Artifacts + team_outcomes$Gini_Consistency)
```

Which is a better predictor of conflict (multiple linear regression: conflict~gini_artifacts + gini_consistency)? 

```{r}
summary(model_conflict)
```

Which is a better predictor of cohesiveness (multiple linear regression: conflict~gini_artifacts + gini_consistency)? 

```{r}
summary(model_cohesiveness)
```
```{r}
sum(is.na(team_outcomes$Conflict)) 
sum(is.na(team_outcomes$Cohesiveness)) 
```
```{r}
library(MASS)
# Convert 'Conflict' to an ordered factor (adjust levels as per your data)
team_outcomes$Conflict <- factor(team_outcomes$Conflict, 
                                 levels = c("Strongly Disagree", "Disagree", "Neutral", "Agree", "Strongly Agree"), 
                                 ordered = TRUE)

# Convert 'Cohesiveness' to an ordered factor (adjust levels as per your data)
team_outcomes$Cohesiveness <- factor(team_outcomes$Cohesiveness, 
                                     levels = c("Strongly Disagree", "Disagree", "Neutral", "Agree", "Strongly Agree"), 
                                     ordered = TRUE)

# Fit the ordinal logistic regression model for Conflict
model_conflict_ordinal <- polr(Conflict ~ Gini_Artifacts + Gini_Consistency, data = team_outcomes, method = "logistic")

# View the summary of the model
summary(model_conflict_ordinal)
```