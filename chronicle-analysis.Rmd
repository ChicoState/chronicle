---
title: "Data Analysis for Chronicle Project"
author: "Kevin Buffardi and Rahul Bijoor"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_float: false
    number_sections: true
    theme: readable
    highlight: textmate
    smart: true
    df_print: tibble
    code_folding: show
---


```{r setup, include=FALSE, error=FALSE, warning=FALSE, message=FALSE}
# Load required packages (hidden from knitted document)
library("readr")
library("tidyr")
library("dplyr")
library("ggplot2")
library("writexl")
library("caret")
library("tibble")
library("psych")
library("stringr")
library("DescTools")

knitr::opts_chunk$set(echo = TRUE)
```

First, we will load the anonymized GitHub and survey data:
```{r loading_data, message = FALSE, warning = FALSE}
github <- readr::read_csv("data/coded_collated_data.csv")
survey <- readr::read_csv("data/coded_survey_anonymous.csv")
```

At a glance, here are some descriptive statistics of our data:

  * Total number of teams: **`r github %>% select(Repository) %>% unique() %>% nrow()`**
  * Total number of students: **`r github %>% select(Author) %>% unique() %>% nrow()`**
  
Summarize team trends:
```{r team trends}
github_by_team <- github %>% select(Repository,Action) %>% pivot_wider(names_from = Action, values_from = Action, values_fn = length)
github_by_team <- github_by_team %>% mutate(total = commit+pull_request+issue+comment+code_review)
```
  * Trends, per team:
    * Commits (M = **`r github_by_team %>% pull(commit) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(commit) %>% sd(na.rm=TRUE)`**)
    * Pull requests (M = **`r github_by_team %>% pull(pull_request) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(pull_request) %>% sd(na.rm=TRUE)`**)
    * Issues (M = **`r github_by_team %>% pull(issue) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(issue) %>% sd(na.rm=TRUE)`**)
    * Comments (M = **`r github_by_team %>% pull(comment) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(comment) %>% sd(na.rm=TRUE)`**)
    * Code Reviews (M = **`r github_by_team %>% pull(code_review) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(code_review) %>% sd(na.rm=TRUE)`**)
    * Total artifacts (M = **`r github_by_team %>% pull(total) %>% mean(na.rm=TRUE)`**, sd = **`r github_by_team %>% pull(total) %>% sd(na.rm=TRUE)`**)
    
For each team, we need to calculate their aggregate score for **Conflict** and **Cohesiveness**, by finding the median score of the answers
of all of the members of the team (who responded).

  * **Conflict**. Conflict questions include:
    * `How frequently do you have disagreements within your work group about the task of the project you are working on?`
    * `How often do people in your work group have conflicting opinions about the project you are working on?`
    * `How much emotional conflict is there in your work group?`
    * `How often do people get angry while working in your group?`
    * `How much conflict of ideas is there in your work group?`
    * `How often do you disagree about resource allocation in your work group?`
    * `How much relationship tension is there in your work group?`
    * `How often are there disagreements about who should do what in your work group?`
    * `How much conflict is there in your group about task responsibilities?`
  * **Cohesiveness**. Cohesiveness questions include:
    * `Team members get to participate in enjoyable activities`
    * `Team members enjoy spending time together`
    * `Team members get along well`
    * `I'm unhappy with my team's level of commitment to the task` (inverse scale should be reversed by `6-x` where x is the raw value)
    * `Our team is united in trying to reach its goals for performance`
    * `Our team members have conflicting aspirations for the team's performance`
    * `Team members like each other`
    * `Team members like the work that the group does`
    * `Being part of the team allows team members to do enjoyable work`

```{r calc_team_survey_scores}
conflict_columns <- c(
  "How frequently do you have disagreements within your work group about the task of the project you are working on?",
  "How often do people in your work group have conflicting opinions about the project you are working on?",
  "How much emotional conflict is there in your work group?",
  "How often do people get angry while working in your group?",
  "How much conflict of ideas is there in your work group?",
  "How often do you disagree about resource allocation in your work group?",
  "How much relationship tension is there in your work group?",
  "How often are there disagreements about who should do what in your work group?",
  "How much conflict is there in your group about task responsibilities?"
)

cohesiveness_columns <- c(
  "Team members get to participate in enjoyable activities",
  "Team members enjoy spending time together",
  "Team members get along well",
  "Our team is united in trying to reach its goals for performance",
  "Our team members have conflicting aspirations for the team's performance",
  "Team members like each other",
  "Team members like the work that the group does",
  "Being part of the team allows team members to do enjoyable work"
)

# Calculate median scores

aggregate_scores <- survey %>%
  group_by(`Your Team`) %>%  # Group by team
  summarize(
    Conflict = median(as.numeric(unlist(across(all_of(conflict_columns)))), na.rm = TRUE),
    Cohesiveness = median(as.numeric(unlist(across(all_of(cohesiveness_columns)))), na.rm = TRUE)
  )

print(aggregate_scores)
```


Then, we want to look at the descriptive statistics for these team conflict and cohesiveness scores:

  * Team conflict (M = `r `, sd = `r `)
  * Team cohesiveness (M = `r `, sd = `r `)
  
Next, we want to determine statistics about individuals:
  * the number of artifacts by each person in the GitHub data (M = `r `, sd = `r `)
  * the ratio of *number of weeks an individual has an artifact* to the *total number of weeks their team had any activity during the semester* (from 0.0 to 1.0), from here on out referred to as **"consistency"**

```{r individuals_artifacts}

```

To determine the (in)equality within each team, we use [`Gini` from `DescTools`](https://search.r-project.org/CRAN/refmans/DescTools/html/Gini.html) to calculate the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient). Low values (approaching 0) indicate equality while high values (approaching 1) represent inequality.

  * Team Gini for total number of artifacts (M = `r `, sd = `r `)
  * Team Gini for consistency (M = `r `, sd = `r `)
  
```{r gini}

```

What is the correlation between:

  * Team Gini for total number of artifacts *vs* Team conflict (rho and p-value): `r `
  * Team Gini for total number of artifacts *vs* Team cohesiveness (rho and p-value): `r `
  * Team Gini for consistency *vs* Team conflict (rho and p-value): `r `
  * Team Gini for consistency *vs* Team cohesiveness (rho and p-value): `r `
  
Which is a better predictor of conflict (multiple linear regression: conflict~gini_artifacts + gini_consistency)? `r `

Which is a better predictor of cohesiveness (multiple linear regression: conflict~gini_artifacts + gini_consistency)? `r `